What libaries do you need to creat a Model?

import torch
import torch.nn as nn
import torch.nn.functional as F

What is an LLM?
A transformer based model trained on test dataset

What is an LLM in practise?
A text completion model that attempts to predict the next token in a sequence based on probabilities 

What is tokenization?
An algorithm that splits raw text into a sequence of words (tokens)
Each word gets feed into the models vocabulary which stores all words that our model needs which it then receives an integer representation (token) each word is converted to an integer 

What are the text tokenizer techniques?
-Tokenization on a word level
-Tokenization on a character level

How to build tokenizer?
From scratch see simple_tokenizer.py

How do I convert a sentence to numeric representation? 
Split the text into a list of words using a tokenizer

What are examples of tokenizers?
TikToken - OpenAI, SentencePiece - Google


Why do we need embeddings?
We cannot feed a model with numerical representations of words and characters directly as the numbers are static and they do not capture any meaning (it only sees a single number for ech words)

What are embeddings?
Vector reprenstation for tokens formed from learnable paramters that are adjusted during training to improve word representation

How do we convert a number to a vector?
Embedding layers is a lookup table with rows and coloumns
The rows correspond to the vocabulary size of our model, encompassing all possible tokens that our models could predict during generation.
The columns refers to the embedding (vector) size of each token.



How do you determine the perfect size of an embedding layer?
Embedding layers has N rows each row has a corresponding vector repreensentation of a numeric token generated by Tokenizers
The size of each vector is what we determibe
Large vectors = Learn + capture more detail but more computationally expensive 
Small vectors = less compuataional expensive 

How does a model understand the meaning of words?
Embedding layers learn how to represent word as continous vector space baded on the text feed into the nerual network

What is a transformer?
A transfomer is an architecture composed of a Encoder and Decoder block using an attention mechanism called Scaled dot product attention (comptes relations bwteen tokens in a context)

What is an encoder?
Encoder block tries to analyze each word considering the entire text context.

What is a decoder?
Decoder block mask all future words at t position and uses only the previous words available to analyze the current one

What is the Attention mechnaism?


What is Multi-Head Attention?

Resources: https://www.manning.com/books/build-a-large-language-model-from-scratch?utm_source=raschka&utm_medium=affiliate&utm_campaign=book_raschka_build_12_12_23&a_aid=raschka&a_bid=4c2437a0&chan=mm_github
https://medium.com/@msouza.os/llm-from-scratch-with-pytorch-9f21808c6319
https://github.com/Exorust/LLM-Cookbook?tab=readme-ov-file#model-creation


To do: create a transcrpit of this: https://www.youtube.com/@AndrejKarpathy/videos

